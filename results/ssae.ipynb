{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "PyTorch Version:  1.11.0\n",
      "Torchvision Version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # Initialization\n",
    "    def __init__(self, data, label, unlabel_idx, mode='2D'):\n",
    "        self.data, self.label, self.mode, self.unlabel_idx= data, label, mode, unlabel_idx\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == '2D':\n",
    "            return self.data[index, :], self.label[index, :], self.unlabel_idx[index, :]\n",
    "        elif self.mode == '3D':\n",
    "            return self.data[:, index, :], self.label[:, index, :], self.unlabel_idx[:, index, :]\n",
    "\n",
    "    # Get length\n",
    "    def __len__(self):\n",
    "        if self.mode == '2D':\n",
    "            return self.data.shape[0]\n",
    "        elif self.mode == '3D':\n",
    "            return self.data.shape[1]\n",
    "\n",
    "    def getNumpyLabel(self):\n",
    "        return self.label.cpu().detach().numpy()\n",
    "    def getTensorTrain(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim, bias=True)\n",
    "        self.y_layer = nn.Linear(latent_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, X, decode=True):\n",
    "        # Return (latent/decode output, Y estimate by dense)\n",
    "        H = torch.sigmoid(self.encoder(X))\n",
    "        if decode:\n",
    "            return torch.sigmoid(self.decoder(H)), torch.sigmoid(self.y_layer(H))\n",
    "        else:\n",
    "            return H, torch.sigmoid(self.y_layer(H))\n",
    "\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self,AE_list):\n",
    "        super().__init__()\n",
    "        self.num_AE = len(AE_list)\n",
    "        self.SAE_list=[]\n",
    "\n",
    "        for i in range(1, self.num_AE+1):\n",
    "            if i != self.num_AE:\n",
    "                self.SAE_list.append(AE(AE_list[i-1], AE_list[i]).to(device))\n",
    "            else:\n",
    "                self.SAE_list.append(AE(AE_list[-1], AE_list[-1]).to(device))\n",
    "        \n",
    "        #self.y_estimate = nn.Linear(AE_list[-1],1)\n",
    "    \n",
    "    def wgtFromList(self, wts_list):\n",
    "        for i in range(self.num_AE):\n",
    "            self.SAE_list[i].load_state_dict(wts_list[i])\n",
    "    \n",
    "    def forward(self, x, layer_idx, preTrain = False):\n",
    "        # preTrain: previous layers' parameters are frozen \n",
    "        # preTrain -> Return (input, AE_output, y estimate)\n",
    "        # !preTrain -> Return last layer's (latent, estimate)\n",
    "        output = x\n",
    "        if preTrain:\n",
    "            if layer_idx == 0:\n",
    "                inputs = output\n",
    "                output, y_estimate = self.SAE_list[layer_idx](output, decode=True)                \n",
    "                return inputs, output, y_estimate\n",
    "\n",
    "            else:\n",
    "                for i in range(layer_idx):\n",
    "                    for param in self.SAE_list[i].parameters():\n",
    "                        param.requires_grad = False\n",
    "                    output,_ = self.SAE_list[i](output, decode = False)\n",
    "                inputs = output\n",
    "                output, y_estimate = self.SAE_list[layer_idx](output, decode=True)                \n",
    "                return inputs, output, y_estimate\n",
    "        else:\n",
    "            for i in range(self.num_AE-1):\n",
    "                for param in self.SAE_list[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                output, _ = self.SAE_list[i](output, decode = False)\n",
    "            return self.SAE_list[-1](output, decode = False)\n",
    "\n",
    "def loss_func(input_encode, output_latent, y_estimate,  y_label, unlabel_idx, tradeoff_param):\n",
    "    N_total = unlabel_idx.size(dim=0)\n",
    "    N_u = torch.sum(unlabel_idx)\n",
    "    N_l = N_total - N_u\n",
    "    label_idx = torch.ones(unlabel_idx.size(), dtype=int, device=device)-unlabel_idx\n",
    "    unlabel_term = (N_total/(2*N_u))*nn.functional.mse_loss(unlabel_idx*input_encode, unlabel_idx*output_latent)\n",
    "    label_term = (N_total/(2*N_l))*(nn.functional.mse_loss(label_idx*input_encode, label_idx*output_latent)+\n",
    "                    tradeoff_param*nn.functional.mse_loss(label_idx*y_estimate, label_idx*y_label))\n",
    "    return unlabel_term+label_term\n",
    "                                \n",
    "\n",
    "def SAE_train(dataloader_dict, SAE_model, unsp_epochs, unsp_lr, tradeoff_param, sp_epochs, sp_lr):\n",
    "    SAE_model.to(device)\n",
    "    for layer_idx in range(SAE_model.num_AE):\n",
    "        print('Pre-training on the {} layer'.format(layer_idx))\n",
    "        # Pretraining of each layer\n",
    "        optimizer = torch.optim.Adam(SAE_model.SAE_list[layer_idx].parameters(), lr = unsp_lr)\n",
    "        #SAE_model.SAE_list[layer_idx].train()\n",
    "        best_layer_wts = copy.deepcopy(SAE_model.SAE_list[layer_idx].state_dict())\n",
    "        best_loss = np.inf\n",
    "\n",
    "        for epoch in range(unsp_epochs):\n",
    "            #print('Epoch {}/{}'.format(epoch, unsp_epochs - 1))\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    SAE_model.SAE_list[layer_idx].train()\n",
    "                else:\n",
    "                    SAE_model.SAE_list[layer_idx].eval()\n",
    "                \n",
    "                for inputs, labels, unlabel_idx in dataloader_dict[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    unlabel_idx = unlabel_idx.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    input_encode,output_latent,y_estimate = SAE_model(inputs,layer_idx,preTrain=True)\n",
    "                    loss = loss_func(input_encode, output_latent, y_estimate, labels, unlabel_idx, tradeoff_param)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                loss= loss.item()\n",
    "                if phase == 'train':\n",
    "                    train_loss = loss\n",
    "                else:\n",
    "                    val_loss = loss\n",
    "            print(f\"Epoch {epoch+1}/{unsp_epochs}, train_loss: {train_loss:>7f}, val_loss: {val_loss:>7f}\")\n",
    "            if phase == 'val' and val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_layer_wts = copy.deepcopy(SAE_model.SAE_list[layer_idx].state_dict())\n",
    "        SAE_model.SAE_list[layer_idx].load_state_dict(best_layer_wts)\n",
    "        print(f'Best loss: {best_loss:>7f}')\n",
    "\n",
    "    print('Start Finetuning\\n')\n",
    "    param_list=[]\n",
    "    best_wts_list=[]\n",
    "    for tmp_sub_model in SAE_model.SAE_list:\n",
    "        best_wts_list.append(tmp_sub_model.state_dict())\n",
    "        for tmp_param in tmp_sub_model.parameters():\n",
    "            param_list.append(tmp_param)\n",
    "    optimizer = torch.optim.Adam(param_list, lr=sp_lr)\n",
    "    #SAE_model.train()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(sp_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            sum_loss=0\n",
    "            if phase == 'train':\n",
    "                SAE_model.train()\n",
    "            else:\n",
    "                SAE_model.eval()\n",
    "\n",
    "            for tmp_inputs, tmp_labels, unlabel_idx in dataloader_dict[phase]:\n",
    "                #inputs = inputs.to(device)\n",
    "                #labels = labels.to(device)\n",
    "                unlabel_idx = unlabel_idx.to(device)\n",
    "                label_idx = torch.ones(unlabel_idx.size(), dtype=int, device=device)-unlabel_idx\n",
    "                inputs = torch.index_select(tmp_inputs, 0, torch.argwhere(label_idx)[:,0]).to(device)\n",
    "                labels = torch.index_select(tmp_labels, 0, torch.argwhere(label_idx)[:,0]).to(device)\n",
    "                optimizer.zero_grad()\n",
    "                _, y_estimate = SAE_model(inputs,SAE_model.num_AE-1,preTrain=False)\n",
    "                #print(y_estimate)\n",
    "                #print('label\\n', labels)\n",
    "                loss = mse_loss(y_estimate, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                sum_loss += loss.detach().item()\n",
    "            if phase == 'train':\n",
    "                train_loss = sum_loss\n",
    "            else:\n",
    "                val_loss = sum_loss\n",
    "        print(f\"Epoch {epoch+1}/{sp_epochs}, train_loss: {train_loss:>7f}, val_loss: {val_loss:>7f}\")\n",
    "        if phase == 'val' and val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            for AE_idx in range(SAE_model.num_AE):\n",
    "                best_wts_list[AE_idx] = copy.deepcopy(SAE_model.SAE_list[AE_idx].state_dict())\n",
    "    SAE_model.wgtFromList(best_wts_list)\n",
    "    print('Training complete')\n",
    "    #torch.save(SAE_model.state_dict(), \"model.pth\")\n",
    "    with open('wts_list', 'wb') as fp:\n",
    "        pickle.dump(best_wts_list, fp)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genUnlabelList(data, label, remain = 0.2, mode = '2D'):\n",
    "    if mode == '2D':\n",
    "        label_idx = np.arange(0, data.shape[0], int(1/remain), dtype=int)\n",
    "        unlabel_idx = np.ones((data.shape[0],1)).astype('int')\n",
    "    elif mode == '3D':\n",
    "        label_idx = np.arange(0, data.shape[1], int(1/remain), dtype=int)\n",
    "        unlabel_idx = np.ones((data.shape[1],1)).astype('int')\n",
    "    unlabel_idx[label_idx]=0\n",
    "    return unlabel_idx\n",
    "\n",
    "data = pd.read_csv('Debutanizer_Column_Data.txt', sep='\\s+')\n",
    "data = data.values\n",
    "\n",
    "x_temp = data[:, :7]\n",
    "y_temp = data[:, 7]\n",
    "\n",
    "\n",
    "x_new = np.zeros([2390, 13])\n",
    "x_6 = x_temp[:, 4]\n",
    "x_9 = (x_temp[:, 5] + x_temp[:, 6])/2\n",
    "x_new[:, :5] = x_temp[4: 2394, :5]\n",
    "\n",
    "x_new[:, 5] = x_6[3: 2393]\n",
    "x_new[:, 6] = x_6[2: 2392]\n",
    "x_new[:, 7] = x_6[1: 2391]\n",
    "x_new[:, 8] = x_9[4: 2394]\n",
    "\n",
    "x_new[:, 9] = y_temp[3: 2393]\n",
    "x_new[:, 10] = y_temp[2: 2392]\n",
    "x_new[:, 11] = y_temp[1:2391]\n",
    "x_new[:, 12] = y_temp[:2390]\n",
    "y_new = y_temp[4: 2394]\n",
    "y_new = y_new.reshape([-1, 1])\n",
    "\n",
    "train_x = x_new[:1000, :]\n",
    "train_y = y_new[:1000]\n",
    "\n",
    "val_x = x_new[1000:1600, :]\n",
    "val_y = y_new[1000:1600]\n",
    "\n",
    "test_x = x_new[1600:2390, :]\n",
    "test_y = y_new[1600:2390]\n",
    "\n",
    "train_unlabel = genUnlabelList(train_x, train_y, remain = 0.2, mode='2D')\n",
    "train_dataset = MyDataset(torch.tensor(train_x, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(train_y, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(train_unlabel, dtype=torch.int, device=device))\n",
    "val_unlabel = genUnlabelList(val_x, val_y, remain = 0.2, mode='2D')\n",
    "val_dataset = MyDataset(torch.tensor(val_x, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(val_y, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(val_unlabel, dtype=torch.int, device=device))\n",
    "test_unlabel = genUnlabelList(test_x, test_y, remain = 0.2, mode='2D')\n",
    "test_dataset = MyDataset(torch.tensor(test_x, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(test_y, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(test_unlabel, dtype=torch.int, device=device))\n",
    "\n",
    "# Key params\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "dataloader_dict ={'train' : train_dataloader, 'val' : val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training on the 0 layer\n",
      "Epoch 1/50, train_loss: 0.010389, val_loss: 0.071277\n",
      "Epoch 2/50, train_loss: 0.008566, val_loss: 0.083016\n",
      "Epoch 3/50, train_loss: 0.007035, val_loss: 0.082533\n",
      "Epoch 4/50, train_loss: 0.006427, val_loss: 0.082096\n",
      "Epoch 5/50, train_loss: 0.006285, val_loss: 0.081615\n",
      "Epoch 6/50, train_loss: 0.006257, val_loss: 0.080887\n",
      "Epoch 7/50, train_loss: 0.006223, val_loss: 0.080653\n",
      "Epoch 8/50, train_loss: 0.006220, val_loss: 0.080443\n",
      "Epoch 9/50, train_loss: 0.006187, val_loss: 0.080075\n",
      "Epoch 10/50, train_loss: 0.006148, val_loss: 0.079691\n",
      "Epoch 11/50, train_loss: 0.006112, val_loss: 0.079318\n",
      "Epoch 12/50, train_loss: 0.006068, val_loss: 0.078916\n",
      "Epoch 13/50, train_loss: 0.006017, val_loss: 0.078494\n",
      "Epoch 14/50, train_loss: 0.005964, val_loss: 0.078069\n",
      "Epoch 15/50, train_loss: 0.005906, val_loss: 0.077622\n",
      "Epoch 16/50, train_loss: 0.005843, val_loss: 0.077181\n",
      "Epoch 17/50, train_loss: 0.005779, val_loss: 0.076749\n",
      "Epoch 18/50, train_loss: 0.005713, val_loss: 0.076336\n",
      "Epoch 19/50, train_loss: 0.005648, val_loss: 0.075950\n",
      "Epoch 20/50, train_loss: 0.005584, val_loss: 0.075580\n",
      "Epoch 21/50, train_loss: 0.005521, val_loss: 0.075205\n",
      "Epoch 22/50, train_loss: 0.005458, val_loss: 0.074816\n",
      "Epoch 23/50, train_loss: 0.005393, val_loss: 0.074385\n",
      "Epoch 24/50, train_loss: 0.005324, val_loss: 0.073907\n",
      "Epoch 25/50, train_loss: 0.005249, val_loss: 0.073381\n",
      "Epoch 26/50, train_loss: 0.005171, val_loss: 0.072823\n",
      "Epoch 27/50, train_loss: 0.005091, val_loss: 0.072281\n",
      "Epoch 28/50, train_loss: 0.005011, val_loss: 0.071767\n",
      "Epoch 29/50, train_loss: 0.004933, val_loss: 0.071306\n",
      "Epoch 30/50, train_loss: 0.004855, val_loss: 0.070883\n",
      "Epoch 31/50, train_loss: 0.004778, val_loss: 0.070511\n",
      "Epoch 32/50, train_loss: 0.004701, val_loss: 0.070177\n",
      "Epoch 33/50, train_loss: 0.004622, val_loss: 0.069871\n",
      "Epoch 34/50, train_loss: 0.004542, val_loss: 0.069595\n",
      "Epoch 35/50, train_loss: 0.004459, val_loss: 0.069337\n",
      "Epoch 36/50, train_loss: 0.004374, val_loss: 0.069104\n",
      "Epoch 37/50, train_loss: 0.004285, val_loss: 0.068878\n",
      "Epoch 38/50, train_loss: 0.004194, val_loss: 0.068669\n",
      "Epoch 39/50, train_loss: 0.004100, val_loss: 0.068461\n",
      "Epoch 40/50, train_loss: 0.004003, val_loss: 0.068278\n",
      "Epoch 41/50, train_loss: 0.003903, val_loss: 0.068085\n",
      "Epoch 42/50, train_loss: 0.003802, val_loss: 0.067912\n",
      "Epoch 43/50, train_loss: 0.003699, val_loss: 0.067744\n",
      "Epoch 44/50, train_loss: 0.003595, val_loss: 0.067569\n",
      "Epoch 45/50, train_loss: 0.003490, val_loss: 0.067414\n",
      "Epoch 46/50, train_loss: 0.003384, val_loss: 0.067262\n",
      "Epoch 47/50, train_loss: 0.003281, val_loss: 0.067119\n",
      "Epoch 48/50, train_loss: 0.003181, val_loss: 0.066978\n",
      "Epoch 49/50, train_loss: 0.003081, val_loss: 0.066841\n",
      "Epoch 50/50, train_loss: 0.002984, val_loss: 0.066714\n",
      "Best loss: 0.066714\n",
      "Pre-training on the 1 layer\n",
      "Epoch 1/50, train_loss: 0.009259, val_loss: 0.022749\n",
      "Epoch 2/50, train_loss: 0.005190, val_loss: 0.019264\n",
      "Epoch 3/50, train_loss: 0.005076, val_loss: 0.019624\n",
      "Epoch 4/50, train_loss: 0.004961, val_loss: 0.018735\n",
      "Epoch 5/50, train_loss: 0.004982, val_loss: 0.018549\n",
      "Epoch 6/50, train_loss: 0.004905, val_loss: 0.018036\n",
      "Epoch 7/50, train_loss: 0.004817, val_loss: 0.017470\n",
      "Epoch 8/50, train_loss: 0.004745, val_loss: 0.016898\n",
      "Epoch 9/50, train_loss: 0.004655, val_loss: 0.016202\n",
      "Epoch 10/50, train_loss: 0.004553, val_loss: 0.015400\n",
      "Epoch 11/50, train_loss: 0.004440, val_loss: 0.014505\n",
      "Epoch 12/50, train_loss: 0.004323, val_loss: 0.013536\n",
      "Epoch 13/50, train_loss: 0.004207, val_loss: 0.012534\n",
      "Epoch 14/50, train_loss: 0.004103, val_loss: 0.011550\n",
      "Epoch 15/50, train_loss: 0.004016, val_loss: 0.010636\n",
      "Epoch 16/50, train_loss: 0.003951, val_loss: 0.009836\n",
      "Epoch 17/50, train_loss: 0.003907, val_loss: 0.009177\n",
      "Epoch 18/50, train_loss: 0.003881, val_loss: 0.008661\n",
      "Epoch 19/50, train_loss: 0.003862, val_loss: 0.008271\n",
      "Epoch 20/50, train_loss: 0.003846, val_loss: 0.007994\n",
      "Epoch 21/50, train_loss: 0.003825, val_loss: 0.007790\n",
      "Epoch 22/50, train_loss: 0.003797, val_loss: 0.007646\n",
      "Epoch 23/50, train_loss: 0.003757, val_loss: 0.007537\n",
      "Epoch 24/50, train_loss: 0.003708, val_loss: 0.007451\n",
      "Epoch 25/50, train_loss: 0.003647, val_loss: 0.007382\n",
      "Epoch 26/50, train_loss: 0.003585, val_loss: 0.007317\n",
      "Epoch 27/50, train_loss: 0.003518, val_loss: 0.007265\n",
      "Epoch 28/50, train_loss: 0.003446, val_loss: 0.007210\n",
      "Epoch 29/50, train_loss: 0.003374, val_loss: 0.007155\n",
      "Epoch 30/50, train_loss: 0.003297, val_loss: 0.007101\n",
      "Epoch 31/50, train_loss: 0.003224, val_loss: 0.007049\n",
      "Epoch 32/50, train_loss: 0.003146, val_loss: 0.006995\n",
      "Epoch 33/50, train_loss: 0.003068, val_loss: 0.006937\n",
      "Epoch 34/50, train_loss: 0.002989, val_loss: 0.006874\n",
      "Epoch 35/50, train_loss: 0.002910, val_loss: 0.006812\n",
      "Epoch 36/50, train_loss: 0.002827, val_loss: 0.006742\n",
      "Epoch 37/50, train_loss: 0.002742, val_loss: 0.006669\n",
      "Epoch 38/50, train_loss: 0.002656, val_loss: 0.006593\n",
      "Epoch 39/50, train_loss: 0.002567, val_loss: 0.006509\n",
      "Epoch 40/50, train_loss: 0.002475, val_loss: 0.006418\n",
      "Epoch 41/50, train_loss: 0.002379, val_loss: 0.006330\n",
      "Epoch 42/50, train_loss: 0.002283, val_loss: 0.006233\n",
      "Epoch 43/50, train_loss: 0.002183, val_loss: 0.006132\n",
      "Epoch 44/50, train_loss: 0.002080, val_loss: 0.006025\n",
      "Epoch 45/50, train_loss: 0.001978, val_loss: 0.005919\n",
      "Epoch 46/50, train_loss: 0.001875, val_loss: 0.005800\n",
      "Epoch 47/50, train_loss: 0.001770, val_loss: 0.005687\n",
      "Epoch 48/50, train_loss: 0.001665, val_loss: 0.005569\n",
      "Epoch 49/50, train_loss: 0.001559, val_loss: 0.005451\n",
      "Epoch 50/50, train_loss: 0.001455, val_loss: 0.005333\n",
      "Best loss: 0.005333\n",
      "Pre-training on the 2 layer\n",
      "Epoch 1/50, train_loss: 0.013648, val_loss: 0.037106\n",
      "Epoch 2/50, train_loss: 0.007669, val_loss: 0.026740\n",
      "Epoch 3/50, train_loss: 0.007034, val_loss: 0.025888\n",
      "Epoch 4/50, train_loss: 0.007010, val_loss: 0.026025\n",
      "Epoch 5/50, train_loss: 0.006929, val_loss: 0.025317\n",
      "Epoch 6/50, train_loss: 0.006775, val_loss: 0.024419\n",
      "Epoch 7/50, train_loss: 0.006639, val_loss: 0.023601\n",
      "Epoch 8/50, train_loss: 0.006506, val_loss: 0.022737\n",
      "Epoch 9/50, train_loss: 0.006359, val_loss: 0.021776\n",
      "Epoch 10/50, train_loss: 0.006201, val_loss: 0.020740\n",
      "Epoch 11/50, train_loss: 0.006033, val_loss: 0.019633\n",
      "Epoch 12/50, train_loss: 0.005853, val_loss: 0.018448\n",
      "Epoch 13/50, train_loss: 0.005655, val_loss: 0.017155\n",
      "Epoch 14/50, train_loss: 0.005436, val_loss: 0.015753\n",
      "Epoch 15/50, train_loss: 0.005193, val_loss: 0.014187\n",
      "Epoch 16/50, train_loss: 0.004931, val_loss: 0.012499\n",
      "Epoch 17/50, train_loss: 0.004669, val_loss: 0.010781\n",
      "Epoch 18/50, train_loss: 0.004440, val_loss: 0.009200\n",
      "Epoch 19/50, train_loss: 0.004262, val_loss: 0.007885\n",
      "Epoch 20/50, train_loss: 0.004136, val_loss: 0.006867\n",
      "Epoch 21/50, train_loss: 0.004052, val_loss: 0.006116\n",
      "Epoch 22/50, train_loss: 0.003994, val_loss: 0.005581\n",
      "Epoch 23/50, train_loss: 0.003956, val_loss: 0.005208\n",
      "Epoch 24/50, train_loss: 0.003917, val_loss: 0.004939\n",
      "Epoch 25/50, train_loss: 0.003879, val_loss: 0.004747\n",
      "Epoch 26/50, train_loss: 0.003836, val_loss: 0.004600\n",
      "Epoch 27/50, train_loss: 0.003788, val_loss: 0.004481\n",
      "Epoch 28/50, train_loss: 0.003738, val_loss: 0.004380\n",
      "Epoch 29/50, train_loss: 0.003677, val_loss: 0.004296\n",
      "Epoch 30/50, train_loss: 0.003617, val_loss: 0.004216\n",
      "Epoch 31/50, train_loss: 0.003553, val_loss: 0.004141\n",
      "Epoch 32/50, train_loss: 0.003487, val_loss: 0.004073\n",
      "Epoch 33/50, train_loss: 0.003418, val_loss: 0.004008\n",
      "Epoch 34/50, train_loss: 0.003350, val_loss: 0.003944\n",
      "Epoch 35/50, train_loss: 0.003277, val_loss: 0.003882\n",
      "Epoch 36/50, train_loss: 0.003204, val_loss: 0.003822\n",
      "Epoch 37/50, train_loss: 0.003129, val_loss: 0.003764\n",
      "Epoch 38/50, train_loss: 0.003056, val_loss: 0.003707\n",
      "Epoch 39/50, train_loss: 0.002984, val_loss: 0.003649\n",
      "Epoch 40/50, train_loss: 0.002904, val_loss: 0.003592\n",
      "Epoch 41/50, train_loss: 0.002825, val_loss: 0.003538\n",
      "Epoch 42/50, train_loss: 0.002744, val_loss: 0.003482\n",
      "Epoch 43/50, train_loss: 0.002663, val_loss: 0.003431\n",
      "Epoch 44/50, train_loss: 0.002588, val_loss: 0.003377\n",
      "Epoch 45/50, train_loss: 0.002504, val_loss: 0.003319\n",
      "Epoch 46/50, train_loss: 0.002421, val_loss: 0.003273\n",
      "Epoch 47/50, train_loss: 0.002341, val_loss: 0.003217\n",
      "Epoch 48/50, train_loss: 0.002258, val_loss: 0.003164\n",
      "Epoch 49/50, train_loss: 0.002174, val_loss: 0.003113\n",
      "Epoch 50/50, train_loss: 0.002093, val_loss: 0.003058\n",
      "Best loss: 0.003058\n",
      "Pre-training on the 3 layer\n",
      "Epoch 1/50, train_loss: 0.022872, val_loss: 0.063105\n",
      "Epoch 2/50, train_loss: 0.012528, val_loss: 0.048645\n",
      "Epoch 3/50, train_loss: 0.011407, val_loss: 0.046311\n",
      "Epoch 4/50, train_loss: 0.011107, val_loss: 0.045843\n",
      "Epoch 5/50, train_loss: 0.010986, val_loss: 0.045006\n",
      "Epoch 6/50, train_loss: 0.010763, val_loss: 0.043344\n",
      "Epoch 7/50, train_loss: 0.010413, val_loss: 0.041117\n",
      "Epoch 8/50, train_loss: 0.009964, val_loss: 0.038416\n",
      "Epoch 9/50, train_loss: 0.009425, val_loss: 0.035201\n",
      "Epoch 10/50, train_loss: 0.008806, val_loss: 0.031515\n",
      "Epoch 11/50, train_loss: 0.008137, val_loss: 0.027516\n",
      "Epoch 12/50, train_loss: 0.007454, val_loss: 0.023431\n",
      "Epoch 13/50, train_loss: 0.006803, val_loss: 0.019491\n",
      "Epoch 14/50, train_loss: 0.006223, val_loss: 0.015912\n",
      "Epoch 15/50, train_loss: 0.005742, val_loss: 0.012850\n",
      "Epoch 16/50, train_loss: 0.005367, val_loss: 0.010355\n",
      "Epoch 17/50, train_loss: 0.005089, val_loss: 0.008400\n",
      "Epoch 18/50, train_loss: 0.004890, val_loss: 0.006925\n",
      "Epoch 19/50, train_loss: 0.004747, val_loss: 0.005832\n",
      "Epoch 20/50, train_loss: 0.004647, val_loss: 0.005031\n",
      "Epoch 21/50, train_loss: 0.004573, val_loss: 0.004451\n",
      "Epoch 22/50, train_loss: 0.004519, val_loss: 0.004024\n",
      "Epoch 23/50, train_loss: 0.004469, val_loss: 0.003714\n",
      "Epoch 24/50, train_loss: 0.004429, val_loss: 0.003485\n",
      "Epoch 25/50, train_loss: 0.004391, val_loss: 0.003309\n",
      "Epoch 26/50, train_loss: 0.004359, val_loss: 0.003176\n",
      "Epoch 27/50, train_loss: 0.004324, val_loss: 0.003070\n",
      "Epoch 28/50, train_loss: 0.004290, val_loss: 0.002985\n",
      "Epoch 29/50, train_loss: 0.004259, val_loss: 0.002915\n",
      "Epoch 30/50, train_loss: 0.004224, val_loss: 0.002855\n",
      "Epoch 31/50, train_loss: 0.004191, val_loss: 0.002804\n",
      "Epoch 32/50, train_loss: 0.004160, val_loss: 0.002761\n",
      "Epoch 33/50, train_loss: 0.004129, val_loss: 0.002720\n",
      "Epoch 34/50, train_loss: 0.004096, val_loss: 0.002684\n",
      "Epoch 35/50, train_loss: 0.004070, val_loss: 0.002655\n",
      "Epoch 36/50, train_loss: 0.004042, val_loss: 0.002626\n",
      "Epoch 37/50, train_loss: 0.004009, val_loss: 0.002597\n",
      "Epoch 38/50, train_loss: 0.003988, val_loss: 0.002576\n",
      "Epoch 39/50, train_loss: 0.003961, val_loss: 0.002554\n",
      "Epoch 40/50, train_loss: 0.003938, val_loss: 0.002536\n",
      "Epoch 41/50, train_loss: 0.003915, val_loss: 0.002518\n",
      "Epoch 42/50, train_loss: 0.003891, val_loss: 0.002501\n",
      "Epoch 43/50, train_loss: 0.003872, val_loss: 0.002485\n",
      "Epoch 44/50, train_loss: 0.003848, val_loss: 0.002474\n",
      "Epoch 45/50, train_loss: 0.003826, val_loss: 0.002460\n",
      "Epoch 46/50, train_loss: 0.003813, val_loss: 0.002445\n",
      "Epoch 47/50, train_loss: 0.003793, val_loss: 0.002435\n",
      "Epoch 48/50, train_loss: 0.003775, val_loss: 0.002423\n",
      "Epoch 49/50, train_loss: 0.003758, val_loss: 0.002415\n",
      "Epoch 50/50, train_loss: 0.003740, val_loss: 0.002404\n",
      "Best loss: 0.002404\n",
      "Start Finetuning\n",
      "\n",
      "Epoch 1/100, train_loss: 0.044049, val_loss: 0.123744\n",
      "Epoch 2/100, train_loss: 0.037262, val_loss: 0.129172\n",
      "Epoch 3/100, train_loss: 0.024229, val_loss: 0.134531\n",
      "Epoch 4/100, train_loss: 0.019700, val_loss: 0.124754\n",
      "Epoch 5/100, train_loss: 0.016877, val_loss: 0.117980\n",
      "Epoch 6/100, train_loss: 0.016533, val_loss: 0.114883\n",
      "Epoch 7/100, train_loss: 0.016702, val_loss: 0.113018\n",
      "Epoch 8/100, train_loss: 0.016703, val_loss: 0.110938\n",
      "Epoch 9/100, train_loss: 0.016731, val_loss: 0.108754\n",
      "Epoch 10/100, train_loss: 0.016841, val_loss: 0.106805\n",
      "Epoch 11/100, train_loss: 0.016988, val_loss: 0.105091\n",
      "Epoch 12/100, train_loss: 0.017152, val_loss: 0.103543\n",
      "Epoch 13/100, train_loss: 0.017285, val_loss: 0.102055\n",
      "Epoch 14/100, train_loss: 0.017413, val_loss: 0.100688\n",
      "Epoch 15/100, train_loss: 0.017583, val_loss: 0.099416\n",
      "Epoch 16/100, train_loss: 0.017693, val_loss: 0.098217\n",
      "Epoch 17/100, train_loss: 0.017748, val_loss: 0.097089\n",
      "Epoch 18/100, train_loss: 0.017715, val_loss: 0.096042\n",
      "Epoch 19/100, train_loss: 0.017536, val_loss: 0.095038\n",
      "Epoch 20/100, train_loss: 0.017181, val_loss: 0.094079\n",
      "Epoch 21/100, train_loss: 0.016700, val_loss: 0.093081\n",
      "Epoch 22/100, train_loss: 0.016114, val_loss: 0.092007\n",
      "Epoch 23/100, train_loss: 0.015433, val_loss: 0.090892\n",
      "Epoch 24/100, train_loss: 0.014826, val_loss: 0.089701\n",
      "Epoch 25/100, train_loss: 0.014268, val_loss: 0.088485\n",
      "Epoch 26/100, train_loss: 0.013745, val_loss: 0.087174\n",
      "Epoch 27/100, train_loss: 0.013363, val_loss: 0.085921\n",
      "Epoch 28/100, train_loss: 0.013072, val_loss: 0.084662\n",
      "Epoch 29/100, train_loss: 0.012784, val_loss: 0.083395\n",
      "Epoch 30/100, train_loss: 0.012585, val_loss: 0.082220\n",
      "Epoch 31/100, train_loss: 0.012442, val_loss: 0.081067\n",
      "Epoch 32/100, train_loss: 0.012330, val_loss: 0.079926\n",
      "Epoch 33/100, train_loss: 0.012255, val_loss: 0.078859\n",
      "Epoch 34/100, train_loss: 0.012176, val_loss: 0.077833\n",
      "Epoch 35/100, train_loss: 0.012136, val_loss: 0.076875\n",
      "Epoch 36/100, train_loss: 0.012075, val_loss: 0.075993\n",
      "Epoch 37/100, train_loss: 0.012027, val_loss: 0.075131\n",
      "Epoch 38/100, train_loss: 0.011975, val_loss: 0.074277\n",
      "Epoch 39/100, train_loss: 0.011911, val_loss: 0.073506\n",
      "Epoch 40/100, train_loss: 0.011822, val_loss: 0.072762\n",
      "Epoch 41/100, train_loss: 0.011770, val_loss: 0.072034\n",
      "Epoch 42/100, train_loss: 0.011688, val_loss: 0.071335\n",
      "Epoch 43/100, train_loss: 0.011554, val_loss: 0.070646\n",
      "Epoch 44/100, train_loss: 0.011463, val_loss: 0.070017\n",
      "Epoch 45/100, train_loss: 0.011333, val_loss: 0.069373\n",
      "Epoch 46/100, train_loss: 0.011205, val_loss: 0.068761\n",
      "Epoch 47/100, train_loss: 0.011082, val_loss: 0.068164\n",
      "Epoch 48/100, train_loss: 0.010950, val_loss: 0.067552\n",
      "Epoch 49/100, train_loss: 0.010797, val_loss: 0.066977\n",
      "Epoch 50/100, train_loss: 0.010672, val_loss: 0.066374\n",
      "Epoch 51/100, train_loss: 0.010501, val_loss: 0.065824\n",
      "Epoch 52/100, train_loss: 0.010342, val_loss: 0.065253\n",
      "Epoch 53/100, train_loss: 0.010205, val_loss: 0.064690\n",
      "Epoch 54/100, train_loss: 0.010086, val_loss: 0.064141\n",
      "Epoch 55/100, train_loss: 0.009957, val_loss: 0.063580\n",
      "Epoch 56/100, train_loss: 0.009807, val_loss: 0.063027\n",
      "Epoch 57/100, train_loss: 0.009693, val_loss: 0.062518\n",
      "Epoch 58/100, train_loss: 0.009573, val_loss: 0.061985\n",
      "Epoch 59/100, train_loss: 0.009443, val_loss: 0.061451\n",
      "Epoch 60/100, train_loss: 0.009342, val_loss: 0.060897\n",
      "Epoch 61/100, train_loss: 0.009275, val_loss: 0.060385\n",
      "Epoch 62/100, train_loss: 0.009184, val_loss: 0.059900\n",
      "Epoch 63/100, train_loss: 0.009098, val_loss: 0.059416\n",
      "Epoch 64/100, train_loss: 0.009060, val_loss: 0.058907\n",
      "Epoch 65/100, train_loss: 0.008974, val_loss: 0.058434\n",
      "Epoch 66/100, train_loss: 0.008925, val_loss: 0.057997\n",
      "Epoch 67/100, train_loss: 0.008871, val_loss: 0.057528\n",
      "Epoch 68/100, train_loss: 0.008799, val_loss: 0.057096\n",
      "Epoch 69/100, train_loss: 0.008736, val_loss: 0.056621\n",
      "Epoch 70/100, train_loss: 0.008686, val_loss: 0.056204\n",
      "Epoch 71/100, train_loss: 0.008665, val_loss: 0.055790\n",
      "Epoch 72/100, train_loss: 0.008587, val_loss: 0.055395\n",
      "Epoch 73/100, train_loss: 0.008548, val_loss: 0.054996\n",
      "Epoch 74/100, train_loss: 0.008504, val_loss: 0.054602\n",
      "Epoch 75/100, train_loss: 0.008434, val_loss: 0.054232\n",
      "Epoch 76/100, train_loss: 0.008359, val_loss: 0.053854\n",
      "Epoch 77/100, train_loss: 0.008323, val_loss: 0.053516\n",
      "Epoch 78/100, train_loss: 0.008249, val_loss: 0.053130\n",
      "Epoch 79/100, train_loss: 0.008218, val_loss: 0.052797\n",
      "Epoch 80/100, train_loss: 0.008159, val_loss: 0.052497\n",
      "Epoch 81/100, train_loss: 0.008085, val_loss: 0.052133\n",
      "Epoch 82/100, train_loss: 0.008070, val_loss: 0.051792\n",
      "Epoch 83/100, train_loss: 0.007983, val_loss: 0.051479\n",
      "Epoch 84/100, train_loss: 0.007952, val_loss: 0.051191\n",
      "Epoch 85/100, train_loss: 0.007901, val_loss: 0.050833\n",
      "Epoch 86/100, train_loss: 0.007867, val_loss: 0.050592\n",
      "Epoch 87/100, train_loss: 0.007796, val_loss: 0.050259\n",
      "Epoch 88/100, train_loss: 0.007777, val_loss: 0.050036\n",
      "Epoch 89/100, train_loss: 0.007685, val_loss: 0.049703\n",
      "Epoch 90/100, train_loss: 0.007675, val_loss: 0.049476\n",
      "Epoch 91/100, train_loss: 0.007589, val_loss: 0.049188\n",
      "Epoch 92/100, train_loss: 0.007555, val_loss: 0.048963\n",
      "Epoch 93/100, train_loss: 0.007475, val_loss: 0.048648\n",
      "Epoch 94/100, train_loss: 0.007445, val_loss: 0.048467\n",
      "Epoch 95/100, train_loss: 0.007396, val_loss: 0.048205\n",
      "Epoch 96/100, train_loss: 0.007343, val_loss: 0.047998\n",
      "Epoch 97/100, train_loss: 0.007291, val_loss: 0.047683\n",
      "Epoch 98/100, train_loss: 0.007273, val_loss: 0.047550\n",
      "Epoch 99/100, train_loss: 0.007212, val_loss: 0.047252\n",
      "Epoch 100/100, train_loss: 0.007179, val_loss: 0.047128\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "myModel = SAE(AE_list=[13,10,7,5])\n",
    "SAE_train(dataloader_dict , myModel, 50, 0.01, 1.6, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.eval()\n",
    "with torch.no_grad():\n",
    "    _,pred = myModel(torch.tensor(test_x, dtype=torch.float32, device=device),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = SAE(AE_list=[13,10,7,5])\n",
    "#torch.save(model.state_dict(), PATH)\n",
    "#loadModel.load_state_dict(torch.load('model.pth'))\n",
    "with open('wts_list', 'rb') as fp:\n",
    "    wts_list = pickle.load(fp)\n",
    "loadModel.wgtFromList(wts_list)\n",
    "loadModel.eval()\n",
    "with torch.no_grad():\n",
    "    _,pred = loadModel(torch.tensor(test_x, dtype=torch.float32, device=device),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x216953bec10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pred.cpu().detach().numpy(), label='Prediction')\n",
    "plt.plot(test_y, label='Actual')\n",
    "plt.xlabel('Number of records')\n",
    "plt.ylabel('y value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16697247],\n",
       "       [0.16367888],\n",
       "       [0.1602487 ],\n",
       "       [0.1582424 ],\n",
       "       [0.1570443 ],\n",
       "       [0.15479179],\n",
       "       [0.1547679 ],\n",
       "       [0.15598922],\n",
       "       [0.15584463],\n",
       "       [0.15703897],\n",
       "       [0.15915777],\n",
       "       [0.16383627],\n",
       "       [0.16700677],\n",
       "       [0.17250859],\n",
       "       [0.17834621],\n",
       "       [0.18713385],\n",
       "       [0.19605279],\n",
       "       [0.20504501],\n",
       "       [0.20952408],\n",
       "       [0.21723898],\n",
       "       [0.22518005],\n",
       "       [0.23545468],\n",
       "       [0.2410065 ],\n",
       "       [0.24419737],\n",
       "       [0.24873731],\n",
       "       [0.2509001 ],\n",
       "       [0.24885268],\n",
       "       [0.25192147],\n",
       "       [0.25753295],\n",
       "       [0.2642149 ],\n",
       "       [0.27009967],\n",
       "       [0.27675261],\n",
       "       [0.27988109],\n",
       "       [0.28342946],\n",
       "       [0.28342662],\n",
       "       [0.28338823],\n",
       "       [0.28115132],\n",
       "       [0.28221239],\n",
       "       [0.28340843],\n",
       "       [0.2853386 ],\n",
       "       [0.28649487],\n",
       "       [0.29006141],\n",
       "       [0.2932166 ],\n",
       "       [0.29326265],\n",
       "       [0.29561323],\n",
       "       [0.29998999],\n",
       "       [0.30900097],\n",
       "       [0.31687329],\n",
       "       [0.32588654],\n",
       "       [0.3368248 ],\n",
       "       [0.35037181],\n",
       "       [0.3605767 ],\n",
       "       [0.37391642],\n",
       "       [0.38616043],\n",
       "       [0.38494393],\n",
       "       [0.37502575],\n",
       "       [0.37623976],\n",
       "       [0.37386699],\n",
       "       [0.36953167],\n",
       "       [0.36717936],\n",
       "       [0.36712708],\n",
       "       [0.36611456],\n",
       "       [0.36842767],\n",
       "       [0.36838279],\n",
       "       [0.37156343],\n",
       "       [0.37385847],\n",
       "       [0.37942336],\n",
       "       [0.38285354],\n",
       "       [0.38382118],\n",
       "       [0.3837791 ],\n",
       "       [0.3837058 ],\n",
       "       [0.38372682],\n",
       "       [0.38295581],\n",
       "       [0.37947563],\n",
       "       [0.37273913],\n",
       "       [0.36258985],\n",
       "       [0.3514893 ],\n",
       "       [0.34148739],\n",
       "       [0.33370655],\n",
       "       [0.32788996],\n",
       "       [0.32588654],\n",
       "       [0.32464617],\n",
       "       [0.32704054],\n",
       "       [0.3289968 ],\n",
       "       [0.32900475],\n",
       "       [0.33460885],\n",
       "       [0.34352778],\n",
       "       [0.35697802],\n",
       "       [0.37493406],\n",
       "       [0.39635662],\n",
       "       [0.41758773],\n",
       "       [0.44181601],\n",
       "       [0.4698686 ],\n",
       "       [0.50232342],\n",
       "       [0.53802237],\n",
       "       [0.56482121],\n",
       "       [0.59267402],\n",
       "       [0.6217592 ],\n",
       "       [0.64612675],\n",
       "       [0.66406689],\n",
       "       [0.67624551],\n",
       "       [0.68178423],\n",
       "       [0.68069333],\n",
       "       [0.66956356],\n",
       "       [0.64946542],\n",
       "       [0.63034802],\n",
       "       [0.60711065],\n",
       "       [0.57130885],\n",
       "       [0.52539034],\n",
       "       [0.47745761],\n",
       "       [0.42938568],\n",
       "       [0.3993215 ],\n",
       "       [0.36024286],\n",
       "       [0.32564038],\n",
       "       [0.29866838],\n",
       "       [0.28310915],\n",
       "       [0.26861272],\n",
       "       [0.25963356],\n",
       "       [0.25838525],\n",
       "       [0.25635792],\n",
       "       [0.25167943],\n",
       "       [0.25182658],\n",
       "       [0.25289649],\n",
       "       [0.25402152],\n",
       "       [0.25529881],\n",
       "       [0.25638406],\n",
       "       [0.25844831],\n",
       "       [0.26401036],\n",
       "       [0.26863374],\n",
       "       [0.27854414],\n",
       "       [0.28991884],\n",
       "       [0.2988991 ],\n",
       "       [0.30336279],\n",
       "       [0.30450599],\n",
       "       [0.30451111],\n",
       "       [0.30329688],\n",
       "       [0.30096558],\n",
       "       [0.29654166],\n",
       "       [0.29424957],\n",
       "       [0.29193135],\n",
       "       [0.28870855],\n",
       "       [0.28634032],\n",
       "       [0.28441016],\n",
       "       [0.28424766],\n",
       "       [0.28327207],\n",
       "       [0.28103282],\n",
       "       [0.27769028],\n",
       "       [0.27076557],\n",
       "       [0.26403649],\n",
       "       [0.25971481],\n",
       "       [0.25653122],\n",
       "       [0.25066236],\n",
       "       [0.24746797],\n",
       "       [0.24283436],\n",
       "       [0.23738484],\n",
       "       [0.23177337],\n",
       "       [0.23185046],\n",
       "       [0.23405978],\n",
       "       [0.24753104],\n",
       "       [0.2610262 ],\n",
       "       [0.27790949],\n",
       "       [0.29134382],\n",
       "       [0.30110195],\n",
       "       [0.3101544 ],\n",
       "       [0.30777879],\n",
       "       [0.29996673],\n",
       "       [0.2876358 ],\n",
       "       [0.26880136],\n",
       "       [0.26107051],\n",
       "       [0.24761516],\n",
       "       [0.23966387],\n",
       "       [0.23298703],\n",
       "       [0.22954605],\n",
       "       [0.22634427],\n",
       "       [0.22518232],\n",
       "       [0.22522209],\n",
       "       [0.22754032],\n",
       "       [0.23081312],\n",
       "       [0.237432  ],\n",
       "       [0.24530713],\n",
       "       [0.25209281],\n",
       "       [0.25867615],\n",
       "       [0.26766611],\n",
       "       [0.27791173],\n",
       "       [0.29016368],\n",
       "       [0.30474977],\n",
       "       [0.32031881],\n",
       "       [0.33936575],\n",
       "       [0.36057357],\n",
       "       [0.38077395],\n",
       "       [0.40194991],\n",
       "       [0.42085027],\n",
       "       [0.43886597],\n",
       "       [0.45340998],\n",
       "       [0.45887771],\n",
       "       [0.45546102],\n",
       "       [0.44321468],\n",
       "       [0.42312872],\n",
       "       [0.40301244],\n",
       "       [0.38068474],\n",
       "       [0.36601002],\n",
       "       [0.35031485],\n",
       "       [0.33696628],\n",
       "       [0.32238306],\n",
       "       [0.30655661],\n",
       "       [0.29111825],\n",
       "       [0.27791972],\n",
       "       [0.26872295],\n",
       "       [0.25967561],\n",
       "       [0.2519209 ],\n",
       "       [0.24986744],\n",
       "       [0.25064361],\n",
       "       [0.24865606],\n",
       "       [0.24407472],\n",
       "       [0.2428764 ],\n",
       "       [0.24275311],\n",
       "       [0.24067067],\n",
       "       [0.23953035],\n",
       "       [0.23595809],\n",
       "       [0.2394644 ],\n",
       "       [0.24393836],\n",
       "       [0.24753104],\n",
       "       [0.2497521 ],\n",
       "       [0.25299877],\n",
       "       [0.25654429],\n",
       "       [0.25967788],\n",
       "       [0.26408365],\n",
       "       [0.26756383],\n",
       "       [0.26990536],\n",
       "       [0.27434291],\n",
       "       [0.27548612],\n",
       "       [0.27866971],\n",
       "       [0.27978166],\n",
       "       [0.28093566],\n",
       "       [0.28327207],\n",
       "       [0.28319001],\n",
       "       [0.28206523],\n",
       "       [0.28086976],\n",
       "       [0.27641455],\n",
       "       [0.27645375],\n",
       "       [0.27525771],\n",
       "       [0.27536282],\n",
       "       [0.27527646],\n",
       "       [0.2752793 ],\n",
       "       [0.27062921],\n",
       "       [0.26504103],\n",
       "       [0.25725511],\n",
       "       [0.25183624],\n",
       "       [0.24221274],\n",
       "       [0.23223757],\n",
       "       [0.22140214],\n",
       "       [0.21083831],\n",
       "       [0.20007163],\n",
       "       [0.18963333],\n",
       "       [0.1809167 ],\n",
       "       [0.17435806],\n",
       "       [0.17005003],\n",
       "       [0.16821136],\n",
       "       [0.16780907],\n",
       "       [0.16918409],\n",
       "       [0.17275802],\n",
       "       [0.17706378],\n",
       "       [0.1816116 ],\n",
       "       [0.18621621],\n",
       "       [0.19200954],\n",
       "       [0.19743576],\n",
       "       [0.20112336],\n",
       "       [0.20470865],\n",
       "       [0.20712972],\n",
       "       [0.2101843 ],\n",
       "       [0.21485769],\n",
       "       [0.21968391],\n",
       "       [0.22564769],\n",
       "       [0.23301771],\n",
       "       [0.24143775],\n",
       "       [0.25022657],\n",
       "       [0.25989208],\n",
       "       [0.26945817],\n",
       "       [0.27786684],\n",
       "       [0.2842363 ],\n",
       "       [0.28773864],\n",
       "       [0.28784943],\n",
       "       [0.28607783],\n",
       "       [0.28317889],\n",
       "       [0.28051294],\n",
       "       [0.27750889],\n",
       "       [0.27494805],\n",
       "       [0.27332868],\n",
       "       [0.27216446],\n",
       "       [0.27179628],\n",
       "       [0.27180423],\n",
       "       [0.27319459],\n",
       "       [0.27550487],\n",
       "       [0.27841687],\n",
       "       [0.2817766 ],\n",
       "       [0.28647044],\n",
       "       [0.29129439],\n",
       "       [0.29308758],\n",
       "       [0.2965286 ],\n",
       "       [0.29978566],\n",
       "       [0.30309745],\n",
       "       [0.30561511],\n",
       "       [0.30757432],\n",
       "       [0.30763163],\n",
       "       [0.30873336],\n",
       "       [0.30994986],\n",
       "       [0.31220274],\n",
       "       [0.31543634],\n",
       "       [0.31777559],\n",
       "       [0.32086198],\n",
       "       [0.32223874],\n",
       "       [0.32451777],\n",
       "       [0.32206826],\n",
       "       [0.32557685],\n",
       "       [0.32565586],\n",
       "       [0.32770701],\n",
       "       [0.32996249],\n",
       "       [0.33340996],\n",
       "       [0.33349405],\n",
       "       [0.3377663 ],\n",
       "       [0.34219026],\n",
       "       [0.34574885],\n",
       "       [0.34810116],\n",
       "       [0.35130805],\n",
       "       [0.35485925],\n",
       "       [0.35472289],\n",
       "       [0.35685472],\n",
       "       [0.35686268],\n",
       "       [0.35684449],\n",
       "       [0.35678654],\n",
       "       [0.35240179],\n",
       "       [0.34889036],\n",
       "       [0.34211412],\n",
       "       [0.3323088 ],\n",
       "       [0.32207053],\n",
       "       [0.31095443],\n",
       "       [0.30194629],\n",
       "       [0.2916745 ],\n",
       "       [0.28412437],\n",
       "       [0.27839698],\n",
       "       [0.27522646],\n",
       "       [0.27401348],\n",
       "       [0.27166766],\n",
       "       [0.27050592],\n",
       "       [0.26847632],\n",
       "       [0.2696826 ],\n",
       "       [0.26850246],\n",
       "       [0.26617262],\n",
       "       [0.26389278],\n",
       "       [0.2607219 ],\n",
       "       [0.25719204],\n",
       "       [0.25499427],\n",
       "       [0.25163795],\n",
       "       [0.24720831],\n",
       "       [0.24494804],\n",
       "       [0.24376304],\n",
       "       [0.24260417],\n",
       "       [0.24251231],\n",
       "       [0.24261675],\n",
       "       [0.24164854],\n",
       "       [0.24165422],\n",
       "       [0.23934111],\n",
       "       [0.23383932],\n",
       "       [0.22603235],\n",
       "       [0.21926403],\n",
       "       [0.2078746 ],\n",
       "       [0.19800909],\n",
       "       [0.18894072],\n",
       "       [0.17869222],\n",
       "       [0.17103981],\n",
       "       [0.16335898],\n",
       "       [0.15776286],\n",
       "       [0.15194627],\n",
       "       [0.14874166],\n",
       "       [0.14637855],\n",
       "       [0.14325008],\n",
       "       [0.14085855],\n",
       "       [0.14094777],\n",
       "       [0.13652608],\n",
       "       [0.13298056],\n",
       "       [0.12959527],\n",
       "       [0.12530997],\n",
       "       [0.12194285],\n",
       "       [0.11741151],\n",
       "       [0.1153001 ],\n",
       "       [0.11183357],\n",
       "       [0.1107665 ],\n",
       "       [0.10952046],\n",
       "       [0.10956762],\n",
       "       [0.11071331],\n",
       "       [0.11303189],\n",
       "       [0.11424612],\n",
       "       [0.11638876],\n",
       "       [0.11747174],\n",
       "       [0.11981894],\n",
       "       [0.12109851],\n",
       "       [0.11874619],\n",
       "       [0.11748992],\n",
       "       [0.11643082],\n",
       "       [0.11635978],\n",
       "       [0.11885132],\n",
       "       [0.12314685],\n",
       "       [0.12982085],\n",
       "       [0.13885227],\n",
       "       [0.14909281],\n",
       "       [0.1581532 ],\n",
       "       [0.16807668],\n",
       "       [0.17712402],\n",
       "       [0.18504123],\n",
       "       [0.19369251],\n",
       "       [0.20138415],\n",
       "       [0.21169572],\n",
       "       [0.2184248 ],\n",
       "       [0.22632325],\n",
       "       [0.23305009],\n",
       "       [0.23970307],\n",
       "       [0.24401677],\n",
       "       [0.24745774],\n",
       "       [0.25085099],\n",
       "       [0.25434935],\n",
       "       [0.2564994 ],\n",
       "       [0.25548744],\n",
       "       [0.25312717],\n",
       "       [0.24873283],\n",
       "       [0.24406165],\n",
       "       [0.23958773],\n",
       "       [0.2360871 ],\n",
       "       [0.23298987],\n",
       "       [0.23178075],\n",
       "       [0.2306188 ],\n",
       "       [0.23062959],\n",
       "       [0.22626587],\n",
       "       [0.22270728],\n",
       "       [0.21957596],\n",
       "       [0.21478439],\n",
       "       [0.2128657 ],\n",
       "       [0.20822037],\n",
       "       [0.20496955],\n",
       "       [0.20505297],\n",
       "       [0.20505524],\n",
       "       [0.20824452],\n",
       "       [0.21409235],\n",
       "       [0.2205828 ],\n",
       "       [0.2286233 ],\n",
       "       [0.23531832],\n",
       "       [0.23977125],\n",
       "       [0.24513952],\n",
       "       [0.24986744],\n",
       "       [0.25644201],\n",
       "       [0.26201201],\n",
       "       [0.26876194],\n",
       "       [0.2742378 ],\n",
       "       [0.2797845 ],\n",
       "       [0.27972655],\n",
       "       [0.28087281],\n",
       "       [0.28086976],\n",
       "       [0.27973962],\n",
       "       [0.27859641],\n",
       "       [0.27753162],\n",
       "       [0.27543896],\n",
       "       [0.27418553],\n",
       "       [0.27188037],\n",
       "       [0.26636552],\n",
       "       [0.26075574],\n",
       "       [0.2540397 ],\n",
       "       [0.24962881],\n",
       "       [0.24850947],\n",
       "       [0.24844867],\n",
       "       [0.25067031],\n",
       "       [0.25059645],\n",
       "       [0.24972597],\n",
       "       [0.24628272],\n",
       "       [0.24623045],\n",
       "       [0.24621965],\n",
       "       [0.24502645],\n",
       "       [0.24397919],\n",
       "       [0.24275179],\n",
       "       [0.2427372 ],\n",
       "       [0.24395142],\n",
       "       [0.25069361],\n",
       "       [0.25740965],\n",
       "       [0.26525583],\n",
       "       [0.27548612],\n",
       "       [0.28531757],\n",
       "       [0.29784737],\n",
       "       [0.30775777],\n",
       "       [0.31249365],\n",
       "       [0.31335902],\n",
       "       [0.31257774],\n",
       "       [0.31142658],\n",
       "       [0.30575658],\n",
       "       [0.3011332 ],\n",
       "       [0.29570528],\n",
       "       [0.29213078],\n",
       "       [0.28773523],\n",
       "       [0.28342662],\n",
       "       [0.27885323],\n",
       "       [0.27565149],\n",
       "       [0.27215824],\n",
       "       [0.27215029],\n",
       "       [0.27338554],\n",
       "       [0.27437416],\n",
       "       [0.27681856],\n",
       "       [0.27998621],\n",
       "       [0.28236977],\n",
       "       [0.2835948 ],\n",
       "       [0.28548803],\n",
       "       [0.28676248],\n",
       "       [0.28896821],\n",
       "       [0.29136484],\n",
       "       [0.29222225],\n",
       "       [0.29458253],\n",
       "       [0.29018755],\n",
       "       [0.28548803],\n",
       "       [0.28352377],\n",
       "       [0.28241977],\n",
       "       [0.2812205 ],\n",
       "       [0.28129759],\n",
       "       [0.27882198],\n",
       "       [0.27885039],\n",
       "       [0.27881118],\n",
       "       [0.27892425],\n",
       "       [0.27679469],\n",
       "       [0.27795665],\n",
       "       [0.27808222],\n",
       "       [0.27561964],\n",
       "       [0.27215824],\n",
       "       [0.26894851],\n",
       "       [0.26779224],\n",
       "       [0.26431206],\n",
       "       [0.26338363],\n",
       "       [0.26236883],\n",
       "       [0.26103642],\n",
       "       [0.26234497],\n",
       "       [0.26231656],\n",
       "       [0.26118074],\n",
       "       [0.26229269],\n",
       "       [0.26335749],\n",
       "       [0.26549447],\n",
       "       [0.26776554],\n",
       "       [0.27021501],\n",
       "       [0.27095708],\n",
       "       [0.2722446 ],\n",
       "       [0.27223154],\n",
       "       [0.27573785],\n",
       "       [0.28348684],\n",
       "       [0.29143586],\n",
       "       [0.30126448],\n",
       "       [0.31041406],\n",
       "       [0.31818183],\n",
       "       [0.32794507],\n",
       "       [0.33289576],\n",
       "       [0.34010101],\n",
       "       [0.34759716],\n",
       "       [0.35346488],\n",
       "       [0.35717869],\n",
       "       [0.35973718],\n",
       "       [0.36076221],\n",
       "       [0.35647348],\n",
       "       [0.34835968],\n",
       "       [0.33923621],\n",
       "       [0.33054856],\n",
       "       [0.32159269],\n",
       "       [0.31400901],\n",
       "       [0.29828429],\n",
       "       [0.28051408],\n",
       "       [0.26557572],\n",
       "       [0.25017767],\n",
       "       [0.23463761],\n",
       "       [0.2182248 ],\n",
       "       [0.2020904 ],\n",
       "       [0.18559406],\n",
       "       [0.16809316],\n",
       "       [0.15154569],\n",
       "       [0.13530902],\n",
       "       [0.12000987],\n",
       "       [0.10805452],\n",
       "       [0.09933276],\n",
       "       [0.09136328],\n",
       "       [0.08558819],\n",
       "       [0.08321881],\n",
       "       [0.08687116],\n",
       "       [0.09578951],\n",
       "       [0.10578288],\n",
       "       [0.11667627],\n",
       "       [0.12927934],\n",
       "       [0.14201768],\n",
       "       [0.15569122],\n",
       "       [0.17022219],\n",
       "       [0.18263776],\n",
       "       [0.19247715],\n",
       "       [0.19922786],\n",
       "       [0.20498138],\n",
       "       [0.20845929],\n",
       "       [0.20915928],\n",
       "       [0.2099527 ],\n",
       "       [0.21048   ],\n",
       "       [0.20900988],\n",
       "       [0.20538823],\n",
       "       [0.20227282],\n",
       "       [0.20025118],\n",
       "       [0.19932558],\n",
       "       [0.20031424],\n",
       "       [0.20500865],\n",
       "       [0.21400315],\n",
       "       [0.23062959],\n",
       "       [0.25073139],\n",
       "       [0.26752403],\n",
       "       [0.27793279],\n",
       "       [0.24981006],\n",
       "       [0.28657896],\n",
       "       [0.28333741],\n",
       "       [0.27429803],\n",
       "       [0.26087677],\n",
       "       [0.25068054],\n",
       "       [0.24193209],\n",
       "       [0.23966387],\n",
       "       [0.23531832],\n",
       "       [0.22940969],\n",
       "       [0.22743521],\n",
       "       [0.2275193 ],\n",
       "       [0.22630507],\n",
       "       [0.22951373],\n",
       "       [0.23061653],\n",
       "       [0.23526321],\n",
       "       [0.23619675],\n",
       "       [0.23415125],\n",
       "       [0.23431943],\n",
       "       [0.23298703],\n",
       "       [0.23863316],\n",
       "       [0.24647931],\n",
       "       [0.2586449 ],\n",
       "       [0.27089174],\n",
       "       [0.28541189],\n",
       "       [0.29806782],\n",
       "       [0.31342983],\n",
       "       [0.32908941],\n",
       "       [0.34252037],\n",
       "       [0.35513709],\n",
       "       [0.36842256],\n",
       "       [0.38189897],\n",
       "       [0.39076275],\n",
       "       [0.39413271],\n",
       "       [0.39170141],\n",
       "       [0.38504847],\n",
       "       [0.3795154 ],\n",
       "       [0.36736288],\n",
       "       [0.35936158],\n",
       "       [0.34929948],\n",
       "       [0.33939984],\n",
       "       [0.33147756],\n",
       "       [0.32478765],\n",
       "       [0.31921538],\n",
       "       [0.31255672],\n",
       "       [0.30585342],\n",
       "       [0.30004514],\n",
       "       [0.29327398],\n",
       "       [0.28893639],\n",
       "       [0.28346866],\n",
       "       [0.27873789],\n",
       "       [0.27558842],\n",
       "       [0.27204287],\n",
       "       [0.26776042],\n",
       "       [0.26541095],\n",
       "       [0.26339669],\n",
       "       [0.26218247],\n",
       "       [0.26227167],\n",
       "       [0.26427286],\n",
       "       [0.26334442],\n",
       "       [0.26102314],\n",
       "       [0.25757783],\n",
       "       [0.25445447],\n",
       "       [0.2519709 ],\n",
       "       [0.25078281],\n",
       "       [0.24763331],\n",
       "       [0.24413438],\n",
       "       [0.24299643],\n",
       "       [0.2420605 ],\n",
       "       [0.24214459],\n",
       "       [0.24101673],\n",
       "       [0.24210539],\n",
       "       [0.23976614],\n",
       "       [0.23850759],\n",
       "       [0.23739791],\n",
       "       [0.23424841],\n",
       "       [0.2306688 ],\n",
       "       [0.22732498],\n",
       "       [0.22286182],\n",
       "       [0.21729182],\n",
       "       [0.21054172],\n",
       "       [0.20505297],\n",
       "       [0.19951706],\n",
       "       [0.19483856],\n",
       "       [0.19353286],\n",
       "       [0.19280101],\n",
       "       [0.19367945],\n",
       "       [0.19715174],\n",
       "       [0.20039549],\n",
       "       [0.20381807],\n",
       "       [0.20825474],\n",
       "       [0.21292301],\n",
       "       [0.21614578],\n",
       "       [0.22080325],\n",
       "       [0.22504369],\n",
       "       [0.22861023],\n",
       "       [0.22954889],\n",
       "       [0.23060573],\n",
       "       [0.2306688 ],\n",
       "       [0.22748009],\n",
       "       [0.2216868 ],\n",
       "       [0.21938675],\n",
       "       [0.2128912 ],\n",
       "       [0.21049172],\n",
       "       [0.20617004],\n",
       "       [0.20029322],\n",
       "       [0.19717174],\n",
       "       [0.19466811],\n",
       "       [0.19362945],\n",
       "       [0.19357206],\n",
       "       [0.19264079],\n",
       "       [0.19159985],\n",
       "       [0.18693442],\n",
       "       [0.18006101],\n",
       "       [0.1722671 ],\n",
       "       [0.16480391],\n",
       "       [0.15805377],\n",
       "       [0.15100761],\n",
       "       [0.14548704],\n",
       "       [0.14101311],\n",
       "       [0.13676472],\n",
       "       [0.13206804],\n",
       "       [0.12888446],\n",
       "       [0.12535996],\n",
       "       [0.12301049],\n",
       "       [0.12298947],\n",
       "       [0.12763899],\n",
       "       [0.13320613],\n",
       "       [0.13886306],\n",
       "       [0.14439613],\n",
       "       [0.15019454],\n",
       "       [0.15352754],\n",
       "       [0.15565145],\n",
       "       [0.15808502],\n",
       "       [0.15905571],\n",
       "       [0.1570259 ],\n",
       "       [0.15582191],\n",
       "       [0.15689977],\n",
       "       [0.15897936],\n",
       "       [0.16014643],\n",
       "       [0.16374195],\n",
       "       [0.16572721],\n",
       "       [0.17041593],\n",
       "       [0.17346315],\n",
       "       [0.17910644],\n",
       "       [0.18149569],\n",
       "       [0.18378762],\n",
       "       [0.1803968 ],\n",
       "       [0.17917782],\n",
       "       [0.17708993],\n",
       "       [0.17474188],\n",
       "       [0.17356544],\n",
       "       [0.17585525],\n",
       "       [0.17719732],\n",
       "       [0.17911724],\n",
       "       [0.18144853],\n",
       "       [0.18364856],\n",
       "       [0.18369064],\n",
       "       [0.17817064],\n",
       "       [0.17033184],\n",
       "       [0.16240953],\n",
       "       [0.15332584],\n",
       "       [0.14428077],\n",
       "       [0.13868749],\n",
       "       [0.13306465],\n",
       "       [0.13185554],\n",
       "       [0.13306465],\n",
       "       [0.13423683],\n",
       "       [0.13543515],\n",
       "       [0.13543288],\n",
       "       [0.13637665],\n",
       "       [0.13661813],\n",
       "       [0.13540674],\n",
       "       [0.13425274],\n",
       "       [0.1307254 ],\n",
       "       [0.12072351],\n",
       "       [0.1105591 ],\n",
       "       [0.09715601],\n",
       "       [0.08692571],\n",
       "       [0.07927329],\n",
       "       [0.0746607 ],\n",
       "       [0.07023902],\n",
       "       [0.0679671 ],\n",
       "       [0.06679066],\n",
       "       [0.06795772],\n",
       "       [0.0691799 ],\n",
       "       [0.07574924],\n",
       "       [0.08375438],\n",
       "       [0.09157523],\n",
       "       [0.09724239],\n",
       "       [0.1027993 ],\n",
       "       [0.10739655],\n",
       "       [0.11051422],\n",
       "       [0.11412281],\n",
       "       [0.11848142],\n",
       "       [0.12078885],\n",
       "       [0.12281104],\n",
       "       [0.12623327],\n",
       "       [0.12631738],\n",
       "       [0.12514234],\n",
       "       [0.12185365],\n",
       "       [0.11721719],\n",
       "       [0.11292449],\n",
       "       [0.11061138],\n",
       "       [0.10816474],\n",
       "       [0.10943635],\n",
       "       [0.11168924],\n",
       "       [0.11608706],\n",
       "       [0.12393606],\n",
       "       [0.13416865],\n",
       "       [0.14399725],\n",
       "       [0.15308721],\n",
       "       [0.1578282 ],\n",
       "       [0.15975835],\n",
       "       [0.15866742],\n",
       "       [0.1542696 ],\n",
       "       [0.14990589],\n",
       "       [0.14519614],\n",
       "       [0.13957102],\n",
       "       [0.13639768],\n",
       "       [0.13412434],\n",
       "       [0.13056802],\n",
       "       [0.13061006],\n",
       "       [0.13178736],\n",
       "       [0.13295215],\n",
       "       [0.13644768],\n",
       "       [0.13970277],\n",
       "       [0.14314724],\n",
       "       [0.1474513 ],\n",
       "       [0.14984282],\n",
       "       [0.15307639],\n",
       "       [0.15538155],\n",
       "       [0.15759467],\n",
       "       [0.15852026],\n",
       "       [0.15853105],\n",
       "       [0.15773103],\n",
       "       [0.15427698],\n",
       "       [0.14863541],\n",
       "       [0.14256542],\n",
       "       [0.13515334],\n",
       "       [0.12619804],\n",
       "       [0.12146841],\n",
       "       [0.11844278],\n",
       "       [0.11445577],\n",
       "       [0.11121424],\n",
       "       [0.10903238],\n",
       "       [0.10616982],\n",
       "       [0.10228224],\n",
       "       [0.09897253],\n",
       "       [0.09539063],\n",
       "       [0.09388039],\n",
       "       [0.09383323],\n",
       "       [0.0972992 ],\n",
       "       [0.10482433],\n",
       "       [0.11716263],\n",
       "       [0.12827479],\n",
       "       [0.13456014],\n",
       "       [0.13772213],\n",
       "       [0.13923921],\n",
       "       [0.13891931],\n",
       "       [0.14008524],\n",
       "       [0.13941819],\n",
       "       [0.13787271],\n",
       "       [0.13671642],\n",
       "       [0.13648177],\n",
       "       [0.13829259],\n",
       "       [0.14218504],\n",
       "       [0.14829903],\n",
       "       [0.1563793 ],\n",
       "       [0.16679183],\n",
       "       [0.17806153],\n",
       "       [0.19065725],\n",
       "       [0.20316201],\n",
       "       [0.21484121],\n",
       "       [0.22760736],\n",
       "       [0.24111786],\n",
       "       [0.25508404],\n",
       "       [0.26743372],\n",
       "       [0.27794585],\n",
       "       [0.28534826],\n",
       "       [0.2888398 ],\n",
       "       [0.2897313 ],\n",
       "       [0.28890116],\n",
       "       [0.28775512],\n",
       "       [0.28591249],\n",
       "       [0.28489254],\n",
       "       [0.28569427],\n",
       "       [0.28621703],\n",
       "       [0.28732899],\n",
       "       [0.29161938],\n",
       "       [0.29866838],\n",
       "       [0.30539749],\n",
       "       [0.31094931],\n",
       "       [0.31411984],\n",
       "       [0.31760229],\n",
       "       [0.32310919],\n",
       "       [0.33220653],\n",
       "       [0.34671648],\n",
       "       [0.36338213],\n",
       "       [0.37784205],\n",
       "       [0.38923376],\n",
       "       [0.39713789],\n",
       "       [0.39706687],\n",
       "       [0.39239917],\n",
       "       [0.38134837],\n",
       "       [0.36679356],\n",
       "       [0.35350068],\n",
       "       [0.33753506],\n",
       "       [0.32075176],\n",
       "       [0.30972713],\n",
       "       [0.29624504],\n",
       "       [0.28495053],\n",
       "       [0.27375258],\n",
       "       [0.26494671],\n",
       "       [0.25704261],\n",
       "       [0.25616929],\n",
       "       [0.25804942],\n",
       "       [0.26055404],\n",
       "       [0.26375067],\n",
       "       [0.26728312],\n",
       "       [0.26615014],\n",
       "       [0.26507004],\n",
       "       [0.25947618],\n",
       "       [0.25153284],\n",
       "       [0.24165933],\n",
       "       [0.23578252],\n",
       "       [0.23028869],\n",
       "       [0.22602212],\n",
       "       [0.22122033],\n",
       "       [0.22125442],\n",
       "       [0.22034416],\n",
       "       [0.21809185],\n",
       "       [0.21804696],\n",
       "       [0.21577078],\n",
       "       [0.21351562],\n",
       "       [0.21570033],\n",
       "       [0.21808901],\n",
       "       [0.22122033],\n",
       "       [0.22248171],\n",
       "       [0.22477892],\n",
       "       [0.22588292],\n",
       "       [0.22702896],\n",
       "       [0.22708123],\n",
       "       [0.22703919],\n",
       "       [0.22479994],\n",
       "       [0.22043337],\n",
       "       [0.21574376],\n",
       "       [0.2113116 ],\n",
       "       [0.20783028],\n",
       "       [0.20691207],\n",
       "       [0.20694904],\n",
       "       [0.20708029],\n",
       "       [0.20785358],\n",
       "       [0.20905473],\n",
       "       [0.21023203],\n",
       "       [0.21125762],\n",
       "       [0.21136785],\n",
       "       [0.21456962],\n",
       "       [0.21688273],\n",
       "       [0.22038905],\n",
       "       [0.22596132],\n",
       "       [0.23044323],\n",
       "       [0.2359007 ],\n",
       "       [0.24168831],\n",
       "       [0.24716449],\n",
       "       [0.25270786],\n",
       "       [0.25495506],\n",
       "       [0.25832218],\n",
       "       [0.26171827],\n",
       "       [0.26375862],\n",
       "       [0.26851837],\n",
       "       [0.27160992],\n",
       "       [0.27401223],\n",
       "       [0.27750264],\n",
       "       [0.27840494],\n",
       "       [0.27747139],\n",
       "       [0.27624637],\n",
       "       [0.27171503],\n",
       "       [0.26724392],\n",
       "       [0.26178645],\n",
       "       [0.2550204 ],\n",
       "       [0.24713502],\n",
       "       [0.2416724 ],\n",
       "       [0.23378932],\n",
       "       [0.22698692],\n",
       "       [0.21920607],\n",
       "       [0.21248303],\n",
       "       [0.2070168 ],\n",
       "       [0.20114552],\n",
       "       [0.19681077],\n",
       "       [0.19241295]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
